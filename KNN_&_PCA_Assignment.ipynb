{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**KNN & PCA**"
      ],
      "metadata": {
        "id": "Z_ztC3ET4hFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?"
      ],
      "metadata": {
        "id": "bsgCZmed4mvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- K-Nearest Neighbors (KNN) is a non-parametric, supervised learning algorithm that stores training data and uses it to predict new data points based on the proximity of their neighbors. For classification, KNN assigns a new data point to the class that is most common among its 'k' nearest neighbors. For regression, it predicts a continuous value by taking the average of the 'k' nearest neighbors' values."
      ],
      "metadata": {
        "id": "_ilc-KQX4sS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n"
      ],
      "metadata": {
        "id": "rdrQZWYm41vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Curse of Dimensionality describes how, as the number of features (dimensions) in a dataset increases, the data becomes sparse, leading to problems like increased data requirements and decreased algorithm performance. This heavily impacts k-Nearest Neighbors (k-NN) because, in high-dimensional spaces, all points tend to become equidistant, making the concept of \"closeness\" less meaningful and reducing the algorithm's ability to find true neighbors. As a result, k-NN needs exponentially more data to maintain performance and becomes more computationally expensive and prone to overfitting in high dimensions."
      ],
      "metadata": {
        "id": "uiiOUBGS46vV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n"
      ],
      "metadata": {
        "id": "_Oqwu6Fz5ABA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Principal Component Analysis (PCA) is a feature extraction technique that reduces dimensionality by creating new, uncorrelated features (principal components) that maximize data variance, while feature selection is a method that selects a subset of the original features that are most relevant to a prediction task."
      ],
      "metadata": {
        "id": "D49mAEiR5ECm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n"
      ],
      "metadata": {
        "id": "SRCB6FLX5I1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Mathematics Behind Principal Component Analysis (PCA ...In PCA, eigenvectors represent the directions of maximum variance in the data (the principal components), while eigenvalues are scalar values indicating the amount of variance along those corresponding eigenvector directions."
      ],
      "metadata": {
        "id": "oAh-V6775MQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n"
      ],
      "metadata": {
        "id": "QU4oGL935RPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PCA complements KNN in a pipeline by performing dimensionality reduction, which mitigates the \"curse of dimensionality\" by transforming high-dimensional data into a lower-dimensional space. This reduces computational complexity, combats overfitting, and can improve KNN's performance by removing noise and redundant information. The resulting lower-dimensional features are then fed into KNN for more efficient and accurate classification or regression."
      ],
      "metadata": {
        "id": "8YMzazls5Vur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases."
      ],
      "metadata": {
        "id": "qjBR2l_P5gJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_no_scale = knn.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scale:.4f}\")\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN with scaling\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxa6S7035pJ-",
        "outputId": "a2f437ce-add9-416f-b398-621c925b2f9c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407\n",
            "Accuracy with scaling: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component."
      ],
      "metadata": {
        "id": "VNwGDVvx4hCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train a PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Print the explained variance ratio of each principal component\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUe86VrM5_AU",
        "outputId": "002b64d6-23ff-4b15-cd24-f1484a03f089"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "[9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n",
            " 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n",
            " 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n",
            " 8.25392788e-08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset"
      ],
      "metadata": {
        "id": "5VBH1EJ76KE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply PCA (retain top 2 components)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN on PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(f\"Accuracy with PCA (2 components): {accuracy_pca:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diKG7TbB6Omk",
        "outputId": "2749929a-ddf0-4e0f-fafe-9bff26da54e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with PCA (2 components): 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "MJC3SJk54gVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN with euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "print(f\"Accuracy with euclidean distance: {accuracy_euclidean:.4f}\")\n",
        "\n",
        "# Train KNN with manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "print(f\"Accuracy with manhattan distance: {accuracy_manhattan:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TYCBml76j-W",
        "outputId": "05c30def-e550-41c5-984e-43f795565c19"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with euclidean distance: 0.9630\n",
            "Accuracy with manhattan distance: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n"
      ],
      "metadata": {
        "id": "J-czoOdJ6vgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To classify cancer types from high-dimensional gene expression data, you would use PCA to find the most informative latent features, determining the number of components to keep by examining the explained variance, then apply a K-Nearest Neighbors (KNN) classifier on the reduced data."
      ],
      "metadata": {
        "id": "QzX-mA-J62Bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1. Use PCA for Dimensionality Reduction\n",
        "Apply PCA:\n",
        "PCA transforms the original, high-dimensional gene expression data into a new set of orthogonal components, called principal components (PCs).\n",
        "Preserve Variance:\n",
        "Each PC captures a certain amount of variance from the original data. The first few PCs capture the majority of the total variance, while later PCs capture less.\n"
      ],
      "metadata": {
        "id": "4VzyNpn07N5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2. Decide How Many Components to Keep\n",
        "Explained Variance Plot:\n",
        "You would generate a plot (known as a \"scree plot\") showing the cumulative or individual variance explained by each principal component.\n",
        "Identify an \"Elbow\":\n",
        "The point where the rate of explained variance drops significantly, often referred to as the \"elbow,\" indicates where subsequent components contribute less to the overall data variance."
      ],
      "metadata": {
        "id": "njypbmPl7Vm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3. Use KNN for Classification Post-Dimensionality Reduction\n",
        "Train KNN on Reduced Data:\n",
        "After applying PCA to transform the data into the selected principal components, the reduced dataset is used to train the KNN classifier.\n",
        "Classification:\n",
        "The KNN algorithm classifies new samples by finding their k nearest neighbors in the reduced feature space and assigning the most frequent class among those neighbors."
      ],
      "metadata": {
        "id": "MgKchrrj7aJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 4. Evaluate the Model\n",
        "Cross-Validation:\n",
        "A robust method like repeated k-fold cross-validation is crucial for ensuring the model's generalization performance, especially with small datasets.\n",
        "Performance Metrics:\n",
        "Evaluate the model using metrics such as:\n",
        "Accuracy: The overall proportion of correct predictions.\n",
        "Precision: The proportion of true positive predictions among all positive predictions."
      ],
      "metadata": {
        "id": "qd0DZ14l7dqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 5. Justify this Pipeline to Stakeholders\n",
        "Addresses Overfitting:\n",
        "Explain that high-dimensional gene expression data with few samples often leads to overfitting with traditional models. PCA reduces the number of features, significantly mitigating this risk."
      ],
      "metadata": {
        "id": "mx2_yDgp7h-o"
      }
    }
  ]
}